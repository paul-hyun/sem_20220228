{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"24.01.transformer-tutorial-answer.ipynb","provenance":[],"collapsed_sections":["JHkHg6XAXoyK","0TyJlt-k7yzW","gfam2ziEnVgU","if2wTo_5oOTo","BHIf3KEV09K3","DMLh4TXbGrHf","ZIX4axZtWSTy","AP6eq87QHWHv","I-tkm2V6Wfwk","itVetPQs1D98","A_kvndtnH-gh","1ihEbeNeXz0W","y6_0vG8SISFB","EfFpvS_bZOcG","J5TmKt9JIXzc","FVwv6yXxd7O-","uuav6G6Bh8L4","ObWGnfj6Ic_M","nu_2I88J-Aa7","MH_M42l5Iezo","_3T0Fq5H-24f","Z4OCTlKPIh0Z","XFm90vYgDq5P","_oruTm0QDzpB","TfVAStGpCcxJ","zv8egauKIl6h","EBP9O8avENiN","VATIJiKwFvoY","9jpGewMmEINq","sgmPfzaBIo_V","wtnO5WUuGWEr","FaJEgxT2nRjm"],"authorship_tag":"ABX9TyMzKZJrJ71ti6ok7TJYZ+sY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn*"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG","executionInfo":{"status":"ok","timestamp":1647860057611,"user_tz":-540,"elapsed":5394,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}}},"source":["# imports\n","import argparse\n","import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import sentencepiece as spm\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from tqdm.notebook import tqdm, trange"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"BF-NLGZTGvTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647860069047,"user_tz":-540,"elapsed":416,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"7bac0aba-f4f8-4928-f495-42a16a7a177d"},"source":["# 환경 설정\n","args = {\n","    # random seed value\n","    \"seed\": 1234\n","}\n","args = argparse.Namespace(**args)\n","\n","print(args)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(seed=1234)\n"]}]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR","executionInfo":{"status":"ok","timestamp":1647860069394,"user_tz":-540,"elapsed":1,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}}},"source":["# random seed 설정\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt"},"source":["# gpu 사용량 확인\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byCIiLJBbFHh","executionInfo":{"status":"ok","timestamp":1647860076835,"user_tz":-540,"elapsed":357,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}}},"source":["# data dir\n","data_dir = '/content/drive/MyDrive/Data/nlp'\n","os.listdir(data_dir)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TyJlt-k7yzW"},"source":["# Vocabulary*"]},{"cell_type":"code","metadata":{"id":"har00GjJ71ZH"},"source":["# vocab loading\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(os.path.join(data_dir, 'kowiki', 'kowiki_32000.model'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfam2ziEnVgU"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"Ao2HNCO7nGYR"},"source":["args.d_model = 8  # d_model: model hidden dim\n","args.n_head = 2  # n_head: multi head attention head number\n","args.d_head = 4  # d_head: multi head attention head dim\n","args.dropout = 0.1  # dropout: dropout rate\n","args.d_ff = 32  # d_ff: feed forward dim\n","args.norm_eps = 1e-9  # norm_eps: layernormal epsilon\n","args.n_layer = 6  # n_layer: layer number\n","args.n_seq = 16  # n_seq: sequence max number\n","args.n_vocab = len(vocab)  # n_vocab: vocab count\n","args.i_pad = vocab.pad_id()  # i_pad: vocab pad id\n","\n","args"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"if2wTo_5oOTo"},"source":["# Inputs"]},{"cell_type":"code","metadata":{"id":"g3vPIEAW4g3Q"},"source":["# 입력 문장\n","sentences = [\n","    ['나는 오늘 행복해', '나도 기분이 매우 좋아'],\n","    # ['나는 오늘 기분이 좋아', '나도 매우 행복하다'],\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1n4AkfH5O1F"},"source":["# train source, target 데이터 생성\n","train_src_ids, tarin_tgt_ids = [], []\n","for pair in sentences:\n","    train_src_ids.append(vocab.encode_as_ids(pair[0]))\n","    tarin_tgt_ids.append(vocab.encode_as_ids(pair[1]))\n","\n","train_src_ids, tarin_tgt_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-CMX8WjcLze"},"source":["# train enc_inputs, dec_inputs, dec_label 생성\n","train_enc_inputs, train_dec_inputs, train_dec_labels = [], [], []\n","for source_id, target_id in zip(train_src_ids, tarin_tgt_ids):\n","    train_enc_inputs.append(source_id)\n","    train_dec_inputs.append([vocab.bos_id()] + target_id)\n","    train_dec_labels.append(target_id + [vocab.eos_id()])\n","\n","train_enc_inputs, train_dec_inputs, train_dec_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfVjB9UVcjXQ"},"source":["# 문장의 길이를 모두 동일하게 변경 (최대길이 5)\n","for row in train_enc_inputs:\n","    row += [0] * (5 - len(row))\n","\n","# 문장의 길이를 모두 동일하게 변경 (최대길이 8)\n","for row in train_dec_inputs:\n","    row += [0] * (8 - len(row))\n","\n","# 문장의 길이를 모두 동일하게 변경 (최대길이 8)\n","for row in train_dec_labels:\n","    row += [0] * (8 - len(row))\n","\n","train_enc_inputs, train_dec_inputs, train_dec_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oA9OvrUgc6p7"},"source":["# numpy array로 변환\n","train_enc_inputs = np.array(train_enc_inputs)\n","train_dec_inputs = np.array(train_dec_inputs)\n","train_dec_labels = np.array(train_dec_labels)\n","\n","train_enc_inputs, train_dec_inputs, train_dec_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPjz5ab4pQxf"},"source":["# embedding with random weight\n","embed_weight = np.random.randint(-90, 100, (args.n_vocab, args.d_model)) / 100\n","\n","embed = tf.keras.layers.Embedding(args.n_vocab, args.d_model, weights=[embed_weight])\n","embed_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmiVm-ZbpWRo"},"source":["# encoder hidden\n","hidden_enc = embed(train_enc_inputs)\n","hidden_enc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYlr1HUOpdPp"},"source":["# decoder hidden\n","hidden_dec = embed(train_dec_inputs)\n","hidden_dec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHIf3KEV09K3"},"source":["# Mask"]},{"cell_type":"markdown","metadata":{"id":"DMLh4TXbGrHf"},"source":["## PAD Mask"]},{"cell_type":"code","metadata":{"id":"dAVy5leVVkjr"},"source":["# inputs\n","tokens = train_enc_inputs\n","i_pad = args.i_pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqkGRkYwVbp8"},"source":["# pad: True, others: False\n","mask = tf.math.equal(tokens, i_pad)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmfu1PvHWD7s"},"source":["# boolean -> float 32\n","mask = tf.cast(mask, tf.float32)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kabadDZCWH-x"},"source":["# expand dimension for Q n_seq\n","mask = tf.expand_dims(mask, axis=1)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIX4axZtWSTy"},"source":["### 실습\n","- 아래 함수를 완성하세요."]},{"cell_type":"code","metadata":{"id":"ctSRBdAeG7xd"},"source":["def get_pad_mask(tokens, i_pad=0):\n","    \"\"\"\n","    pad mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: pad mask (pad: 1, other: 0)\n","    \"\"\"\n","    # pad: True, others: False\n","    mask = tf.math.equal(tokens, i_pad)\n","    # boolean -> float 32\n","    mask = tf.cast(mask, tf.float32)\n","    # expand dimension for Q n_seq\n","    mask = tf.expand_dims(mask, axis=1)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqJIRPejHOuX"},"source":["enc_pad_mask = get_pad_mask(train_enc_inputs)\n","enc_pad_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AP6eq87QHWHv"},"source":["## Causal Mask"]},{"cell_type":"code","metadata":{"id":"jzsEVoZGWbCb"},"source":["# inputs\n","tokens = train_dec_inputs\n","i_pad = args.i_pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3I0gtlAHWdjS"},"source":["# n_seq 조회\n","n_seq = tf.shape(tokens)[1]\n","n_seq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96CeSbSZWfTB"},"source":["# all one mask\n","mask = tf.ones((n_seq, n_seq))\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOwjq_iiWx3E"},"source":["# make reverse causal mask\n","mask = tf.linalg.band_part(mask, -1, 0)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gAAHu3EW-ol"},"source":["# 0 -> 1, 1 -> 0\n","mask = 1 - mask\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25-tgmhWXHSq"},"source":["# expand dim for bs\n","mask = tf.expand_dims(mask, axis=0)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY2cu77GXKqW"},"source":["# get pad_mask\n","pad_mask = get_pad_mask(tokens, i_pad)\n","pad_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BskBMff2XYPr"},"source":["# mask all causal_mask or pad_mask\n","mask = tf.maximum(mask, pad_mask)\n","mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-tkm2V6Wfwk"},"source":["### 실습\n","- 아래 함수를 완성하세요."]},{"cell_type":"code","metadata":{"id":"jgCB3zk-HykT"},"source":["def get_causal_mask(tokens, i_pad=0):\n","    \"\"\"\n","    causal mask 계산하는 함수\n","    :param tokens: tokens (bs, n_seq)\n","    :param i_pad: id of pad\n","    :return mask: causal and pad mask (causal or pad: 1, other: 0)\n","    \"\"\"\n","    # n_seq 조회\n","    n_seq = tf.shape(tokens)[1]\n","    # all one mask\n","    mask = tf.ones((n_seq, n_seq))\n","    # make reverse causal mask\n","    mask = tf.linalg.band_part(mask, -1, 0)\n","    # 0 -> 1, 1 -> 0\n","    mask = 1 - mask\n","    # expand dim for bs\n","    mask = tf.expand_dims(mask, axis=0)\n","    # get pad_mask\n","    pad_mask = get_pad_mask(tokens, i_pad)\n","    # mask all causal_mask or pad_mask\n","    mask = tf.maximum(mask, pad_mask)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUsByA_eH296","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625932108441,"user_tz":-540,"elapsed":53,"user":{"displayName":"현청천","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GingwsOzB_fFlwKa3Z4CH1TlVnyWMq-xfwt25iMhS0=s64","userId":"02662570985009482782"}},"outputId":"39fc7c01-1cb2-4c9f-ca9d-f9f8cedd6a04"},"source":["dec_causal_mask = get_causal_mask(train_dec_inputs)\n","dec_causal_mask"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=\n","array([[[0., 1., 1., 1., 1., 1., 1., 1.],\n","        [0., 0., 1., 1., 1., 1., 1., 1.],\n","        [0., 0., 0., 1., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 0., 1., 1., 1.],\n","        [0., 0., 0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"itVetPQs1D98"},"source":["## Mask 생성"]},{"cell_type":"code","metadata":{"id":"BRfuGqhx1Ddw"},"source":["# Encoder Self Attetnion mask\n","enc_self_mask = get_pad_mask(train_enc_inputs)\n","enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGIvBeLy1C5P"},"source":["# Decoder Self Attetnion mask\n","dec_self_mask = get_causal_mask(train_dec_inputs)\n","dec_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbU42aps1Vy3"},"source":["# Encoder-Decoder Attetnion mask\n","enc_dec_mask = get_pad_mask(train_enc_inputs)\n","enc_dec_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_kvndtnH-gh"},"source":["# Scaled dot product attention"]},{"cell_type":"code","metadata":{"id":"votqlCWcXuYG"},"source":["Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","attn_mask = enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqQPvXGrX5Ys"},"source":["# matmul Q, K.T\n","attn_score = tf.matmul(Q, K, transpose_b=True)\n","attn_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za3S4YllYBSr"},"source":["# d_k\n","d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n","d_k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AU-xuS21YeMT"},"source":["# scale = d_k ** 0.5\n","scale = tf.math.sqrt(d_k)\n","scale"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gW6OgBT5Yp0j"},"source":["# divide by scale\n","attn_scale = tf.math.divide(attn_score, scale)\n","attn_scale"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uF-n1sFsYtLg"},"source":["# do mask (subtract 1e-9 for masked value)\n","attn_scale -= 1.e9 * attn_mask\n","attn_scale"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrahttiTYz4H"},"source":["# calculate attention prob\n","attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n","attn_prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfUnfGt8Y6Bm"},"source":["# weighted sum of V\n","attn_out = tf.matmul(attn_prob, V)\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ihEbeNeXz0W"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"IFkp9rLd_Etz"},"source":["class ScaleDotProductAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Scale Dot Product Attention Class\n","    \"\"\"\n","    def __init__(self, name=\"scale_dot_product_attention\"):\n","        \"\"\"\n","        생성자\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        Q, K, V, attn_mask = inputs\n","        # matmul Q, K.T\n","        attn_score = tf.matmul(Q, K, transpose_b=True)\n","        # d_k\n","        d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n","        # scale = d_k ** 0.5\n","        scale = tf.math.sqrt(d_k)\n","        # divide by scale\n","        attn_scale = tf.math.divide(attn_score, scale)\n","        # do mask (subtract 1e-9 for masked value)\n","        attn_scale -= 1.e9 * attn_mask\n","        # calculate attention prob\n","        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n","        # weighted sum of V\n","        attn_out = tf.matmul(attn_prob, V)\n","        return attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlFeIqeUAlOd"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = ScaleDotProductAttention()\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pU-46iGWeIM3"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxxgRGhJfdAj"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y6_0vG8SISFB"},"source":["# Multi Head Attention"]},{"cell_type":"code","metadata":{"id":"u0kH8PQsb_Fn"},"source":["# Q, K, V input dense layer\n","W_Q = tf.keras.layers.Dense(args.n_head * args.d_head)\n","W_K = tf.keras.layers.Dense(args.n_head * args.d_head)\n","W_V = tf.keras.layers.Dense(args.n_head * args.d_head)\n","# Scale Dot Product Attention class\n","attention = ScaleDotProductAttention(name=\"self_attention\")\n","# output dense layer\n","W_O = tf.keras.layers.Dense(args.d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-SuLmjPb_Fy"},"source":["Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","attn_mask = enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrZMRUJUb_Fz"},"source":["# split\n","Q_m = W_Q(Q)  # (bs, Q_len, d_model) -> (bs, Q_len, n_head * d_head)\n","Q_m = tf.reshape(Q_m, [-1, tf.shape(Q)[1], args.n_head, args.d_head])  # (bs, Q_len, n_head * d_head) -> (bs, Q_len, n_head,  d_head)\n","Q_m = tf.transpose(Q_m, [0, 2, 1, 3])  # (bs, Q_len, n_head,  d_head) -> (bs, n_head, Q_len,  d_head)\n","Q_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q78csQ0MZ4dC"},"source":["# build multihead Q\n","Q_m = tf.transpose(tf.reshape(W_Q(Q), [-1, tf.shape(Q)[1], args.n_head, args.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","Q_m.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muutyQMPb1Fu"},"source":["# build multihead K\n","K_m = tf.transpose(tf.reshape(W_K(K), [-1, tf.shape(K)[1], args.n_head, args.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","K_m.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIpSZbn5byPr"},"source":["# build multihead V\n","V_m = tf.transpose(tf.reshape(W_V(V), [-1, tf.shape(V)[1], args.n_head, args.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","V_m.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ByOiaiecJAy"},"source":["# build multihead mask\n","attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n","attn_mask_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zw_-yn7PcSJQ"},"source":["# Scale Dot Product Attention with multi head Q, K, V, attn_mask\n","attn_out_m = attention((Q_m, K_m, V_m, attn_mask_m))  # (bs, n_head, Q_len, d_head)\n","attn_out_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZn-k8DNcfMv"},"source":["# transpose\n","attn_out_t = tf.transpose(attn_out_m, perm=[0, 2, 1, 3])  # (bs, n_head, Q_len, d_head) -> (bs, Q_len, n_head, d_head)\n","attn_out_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhEdXmIockt1"},"source":["# reshape\n","attn_out_c = tf.reshape(attn_out_t, [-1, tf.shape(Q)[1], args.n_head * args.d_head])  # (bs, Q_len, n_head, d_head) -> (bs, Q_len, n_head * d_head)\n","attn_out_c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcgYMXNPcwN0"},"source":["# linear for output\n","attn_out = W_O(attn_out_c) # (bs, Q_len, n_head * d_head) -> (bs, Q_len, d_model)\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfFpvS_bZOcG"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"3dkSuTsQXJTZ"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Multi Head Attention Class\n","    \"\"\"\n","    def __init__(self, args, name=\"multi_head_attention\"):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.d_model = args.d_model\n","        self.n_head = args.n_head\n","        self.d_head = args.d_head\n","\n","        # Q, K, V input dense layer\n","        self.W_Q = tf.keras.layers.Dense(self.n_head * self.d_head)\n","        self.W_K = tf.keras.layers.Dense(self.n_head * self.d_head)\n","        self.W_V = tf.keras.layers.Dense(self.n_head * self.d_head)\n","        # Scale Dot Product Attention class\n","        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n","        # output dense layer\n","        self.W_O = tf.keras.layers.Dense(self.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: Q, K, V, attn_mask tuple\n","        :return attn_out: attention 실행 결과\n","        \"\"\"\n","        Q, K, V, attn_mask = inputs\n","        # build multihead Q, K, V\n","        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [-1, tf.shape(Q)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n","        K_m = tf.transpose(tf.reshape(self.W_K(K), [-1, tf.shape(K)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        V_m = tf.transpose(tf.reshape(self.W_V(V), [-1, tf.shape(V)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n","        # build multihead mask\n","        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n","        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n","        attn_out_m = self.attention((Q_m, K_m, V_m, attn_mask_m))  # (bs, n_head, Q_len, d_head)\n","        # transpose\n","        attn_out_t = tf.transpose(attn_out_m, perm=[0, 2, 1, 3])   # (bs, n_head, Q_len, d_head) -> (bs, Q_len, n_head, d_head)\n","        # reshape\n","        attn_out_c = tf.reshape(attn_out_t, [-1, tf.shape(Q)[1], self.n_head * self.d_head])  # (bs, Q_len, n_head, d_head) -> (bs, Q_len, n_head * d_head)\n","        # linear for output\n","        attn_out = self.W_O(attn_out_c) # (bs, Q_len, n_head * d_head) -> (bs, Q_len, d_model)\n","        return attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYJFgcWCzjka"},"source":["# Encoder Self Attetnion\n","Q = hidden_enc\n","K = hidden_enc\n","V = hidden_enc\n","\n","attention = MultiHeadAttention(args)\n","attn_out = attention((Q, K, V, enc_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sD7ckaZWznSu"},"source":["# Decoder Self Attetnion\n","Q = hidden_dec\n","K = hidden_dec\n","V = hidden_dec\n","\n","attn_out = attention((Q, K, V, dec_self_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vM9SlWCTzuI0"},"source":["# Encoder-Decoder Attetnion\n","Q = hidden_dec\n","K = hidden_enc\n","V = hidden_enc\n","\n","attn_out = attention((Q, K, V, enc_dec_mask))\n","attn_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J5TmKt9JIXzc"},"source":["# Feed Forward"]},{"cell_type":"code","metadata":{"id":"-yZG1Cmrd7zT"},"source":["W_1 = tf.keras.layers.Dense(args.d_ff, activation=tf.nn.relu)\n","W_2 = tf.keras.layers.Dense(args.d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhc1kdnGefLp"},"source":["inputs = hidden_enc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMJcqtbkeXyU"},"source":[" # linear W_1 and W_2\n","ff_val = W_1(inputs)\n","ff_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bmlw49ffeldb"},"source":["ff_val = W_2(ff_val)\n","ff_val"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVwv6yXxd7O-"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"1MPtv3zPbxVm"},"source":["class PositionWiseFeedForward(tf.keras.layers.Layer):\n","    \"\"\"\n","    Position Wise Feed Forward Class\n","    \"\"\"\n","    def __init__(self, args, name=\"feed_forward\"):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.W_1 = tf.keras.layers.Dense(args.d_ff, activation=tf.nn.relu)\n","        self.W_2 = tf.keras.layers.Dense(args.d_model)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: inputs\n","        :return ff_val: feed forward 실행 결과\n","        \"\"\"\n","        # linear W_1 and W_2\n","        ff_val = self.W_1(inputs)\n","        ff_val = self.W_2(ff_val)\n","        return ff_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQdqftXNb1al"},"source":["# feed-forward class 동작 확인\n","feed_forward = PositionWiseFeedForward(args)\n","ff_val = feed_forward(hidden_enc)\n","ff_val.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuav6G6Bh8L4"},"source":["# LayerNormal\n","- https://arxiv.org/abs/1607.06450"]},{"cell_type":"code","metadata":{"id":"mzW12eb4h_-4"},"source":["# 큰 hidden 생성\n","hidden = np.array([[1, 2, 3],\n","                   [11, 22, 33],\n","                   [111, 222, 333]]).astype(np.float32)\n","hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MX2ETxuBiVIy"},"source":["# layer_normal 실행\n","layer_norm = tf.keras.layers.LayerNormalization()\n","layer_norm(hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDXDxzWjijuo"},"source":["# weights\n","layer_norm.get_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8TEhPFuiqcj"},"source":["# 평균 값\n","mean = np.mean(hidden, axis=-1, keepdims=True)\n","mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD3GOLr1lV_X"},"source":["# sqrt(var - epsiolon)\n","sigma = np.sqrt(np.var(hidden, axis=-1, keepdims=True) + 0.001)\n","sigma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLXZDyCAjAI4"},"source":["# layer normal 계산\n","(hidden - mean) / sigma"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObWGnfj6Ic_M"},"source":["# Encoder Layer"]},{"cell_type":"code","metadata":{"id":"TFiKcwmD-Dgp"},"source":["self_attention = MultiHeadAttention(args)\n","norm1 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","ffn = PositionWiseFeedForward(args)\n","norm2 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","dropout = tf.keras.layers.Dropout(args.dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yd7LAJn6-PIT"},"source":["enc_hidden = hidden_enc\n","self_mask = enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OUd5dBw-LAS"},"source":["# self attention\n","self_attn_val = self_attention((enc_hidden, enc_hidden, enc_hidden, self_mask))\n","self_attn_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfvxqiUm-jAs"},"source":["# add and layer normal\n","norm1_val = norm1(enc_hidden + dropout(self_attn_val))\n","norm1_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdQb3TDj-o8e"},"source":["# feed forward\n","ffn_val = ffn(norm1_val)\n","ffn_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwr0_IUr-kv8"},"source":["# add and layer normal\n","enc_out = norm2(norm1_val + dropout(ffn_val))\n","enc_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nu_2I88J-Aa7"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"TOJHTsjinOry"},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Encoder Layer Class\n","    \"\"\"\n","    def __init__(self, args, name='encoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(args)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(args)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(args.dropout)\n"," \n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_hidden, self_mask tuple\n","        :return enc_out: EncoderLayer 실행 결과\n","        \"\"\"\n","        enc_hidden, self_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((enc_hidden, enc_hidden, enc_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(enc_hidden + self.dropout(self_attn_val))\n","        \n","        # feed forward\n","        ffn_val = self.ffn(norm1_val)\n","        # add and layer normal\n","        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n","\n","        return enc_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heThi4mtoGhW"},"source":["# EncoderLayer 기능 확인\n","encoder_layer = EncoderLayer(args)\n","enc_out = encoder_layer((hidden_enc, enc_self_mask))\n","enc_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH_M42l5Iezo"},"source":["# Decoder Layer"]},{"cell_type":"code","metadata":{"id":"8vf4W6ey-4XJ"},"source":["self_attention = MultiHeadAttention(args)\n","norm1 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","ende_attn = MultiHeadAttention(args)\n","norm2 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","ffn = PositionWiseFeedForward(args)\n","norm3 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","dropout = tf.keras.layers.Dropout(args.dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7YpzDha_Dmb"},"source":["dec_hidden = hidden_dec\n","self_mask = dec_self_mask\n","ende_mask = enc_dec_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MWY556G_Ovd"},"source":["# self attention\n","self_attn_val = self_attention((dec_hidden, dec_hidden, dec_hidden, self_mask))\n","self_attn_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gt0fIb-O_b5S"},"source":["# add and layer normal\n","norm1_val = norm1(dec_hidden + dropout(self_attn_val))\n","norm1_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8AG1eB3U_hPJ"},"source":["# encoder and decoder attention\n","ende_attn_val = ende_attn((norm1_val, enc_out, enc_out, ende_mask))\n","ende_attn_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0fdImYz_mSg"},"source":["# add and layer normal\n","norm2_val = norm2(norm1_val + dropout(ende_attn_val))\n","norm2_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDnHehL__qah"},"source":["# feed forward\n","ffn_val = ffn(norm2_val)\n","ffn_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFgg3mYi-_9W"},"source":["# add and layer normal\n","dec_out = norm3(norm2_val + dropout(ffn_val))\n","dec_out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3T0Fq5H-24f"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"O2ZJaGlqsxI6"},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Decoder Layer Class\n","    \"\"\"\n","    def __init__(self, args, name='decoder_layer'):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.self_attention = MultiHeadAttention(args)\n","        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","        self.ende_attn = MultiHeadAttention(args)\n","        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","        self.ffn = PositionWiseFeedForward(args)\n","        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=args.norm_eps)\n","\n","        self.dropout = tf.keras.layers.Dropout(args.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: dec_hidden, enc_out, self_mask, ende_mask tuple\n","        :return dec_out: DecoderLayer 실행 결과\n","        \"\"\"\n","        dec_hidden, enc_out, self_mask, ende_mask = inputs\n","        # self attention\n","        self_attn_val = self.self_attention((dec_hidden, dec_hidden, dec_hidden, self_mask))\n","        # add and layer normal\n","        norm1_val = self.norm1(dec_hidden + self.dropout(self_attn_val))\n","\n","        # encoder and decoder attention\n","        ende_attn_val = self.ende_attn((norm1_val, enc_out, enc_out, ende_mask))\n","        # add and layer normal\n","        norm2_val = self.norm2(norm1_val + self.dropout(ende_attn_val))\n","\n","        # feed forward\n","        ffn_val = self.ffn(norm2_val)\n","        # add and layer normal\n","        dec_out = self.norm3(norm2_val + self.dropout(ffn_val))\n","\n","        return dec_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEAxwUw9tGDk"},"source":["# Decoder 실행\n","decoder_layer = DecoderLayer(args)\n","dec_out = decoder_layer((hidden_dec, enc_out, dec_self_mask, enc_dec_mask))\n","dec_out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4OCTlKPIh0Z"},"source":["# Weight Shared Embedding"]},{"cell_type":"code","metadata":{"id":"3wVaWDYECzTB"},"source":["initializer = tf.keras.initializers.TruncatedNormal(stddev=args.d_model ** -0.5)\n","shared_weights = initializer(shape=(args.n_vocab, args.d_model))\n","shared_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFm90vYgDq5P"},"source":["### embedding"]},{"cell_type":"code","metadata":{"id":"JhXvfUFcCd3R"},"source":["inputs = train_enc_inputs\n","inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA9nP4TnDZbq"},"source":["# lookup by gather\n","embed = tf.gather(shared_weights, tf.cast(inputs, tf.int32))\n","embed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnQrzVwFDVmQ"},"source":["# muliply d_model ** 0.5\n","embed *= args.d_model ** 0.5\n","embed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_oruTm0QDzpB"},"source":["### linear"]},{"cell_type":"code","metadata":{"id":"c0r2l2KUD6aA"},"source":["inputs = hidden_dec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WvswrZRD1n5"},"source":["# matmul inputs, shared_weights (transpose_b=True)\n","outputs = tf.matmul(inputs, shared_weights, transpose_b=True)\n","outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TfVAStGpCcxJ"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"mjoffNaW4J_W"},"source":["class SharedEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Weighed Shaed Embedding Class\n","    \"\"\"\n","    def __init__(self, args, name='weight_shared_embedding'):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.n_vocab = args.n_vocab\n","        self.d_model = args.d_model\n","    \n","    def build(self, input_shape):\n","        \"\"\"\n","        shared weight 생성\n","        :param input_shape: Tensor Shape (not used)\n","        \"\"\"\n","        with tf.name_scope('shared_embedding_weight'):\n","            self.shared_weights = self.add_weight(\n","                'weights',\n","                shape=[self.n_vocab, self.d_model],\n","                initializer=tf.keras.initializers.TruncatedNormal(stddev=self.d_model ** -0.5)\n","            )\n","\n","    def call(self, inputs, mode='embedding'):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :param mode: 실행 모드\n","        :return: embedding or linear 실행 결과\n","        \"\"\"\n","        # mode가 embedding일 경우 embedding lookup 실행\n","        if mode == 'embedding':\n","            return self._embedding(inputs)\n","        # mode가 linear일 경우 linear 실행\n","        elif mode == 'linear':\n","            return self._linear(inputs)\n","        # mode가 기타일 경우 오류 발생\n","        else:\n","            raise ValueError(f'mode {mode} is not valid.')\n","    \n","    def _embedding(self, inputs):\n","        \"\"\"\n","        embedding lookup\n","        :param inputs: 입력\n","        \"\"\"\n","        # lookup by gather\n","        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n","        # muliply d_model ** 0.5\n","        embed *= self.d_model ** 0.5\n","        return embed\n","\n","    def _linear(self, inputs):  # (bs, n_seq, d_model)\n","        \"\"\"\n","        linear 실행\n","        :param inputs: 입력\n","        \"\"\"\n","        # matmul inputs, shared_weights (transpose_b=True)\n","        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kFRtg0U7o5W"},"source":["embedding = SharedEmbedding(args)\n","hidden_dec = embedding(train_dec_inputs)\n","hidden_dec.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttHI4hYM7os_"},"source":["linear_outputs = embedding(hidden_dec, mode=\"linear\")\n","linear_outputs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv8egauKIl6h"},"source":["# Postional Encoding"]},{"cell_type":"markdown","metadata":{"id":"EBP9O8avENiN"},"source":["### Sinusoid encoding"]},{"cell_type":"code","metadata":{"id":"uA9Dn5jQEYXR"},"source":["# calculate exps\n","exs = np.array([2 * (i_ang // 2) / args.d_model for i_ang in range(args.d_model)])\n","exs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyZJm3pnE6Aa"},"source":["# calculate power\n","angles = np.power(10000, exs)\n","angles"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opgDBktrE9hg"},"source":["# make position\n","pos = np.array([[i] for i in range(n_seq)])\n","pos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxNrYZZPFJPl"},"source":["# position angle\n","pos_encoding = pos / angles\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HeSbdhvEFTR7"},"source":["# sin even number\n","pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWJFTj8-FozO"},"source":["# cos odd number\n","pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2luFEVGGGcT"},"source":["# make embedding with sinusoid encoding\n","embedding = tf.keras.layers.Embedding(args.n_seq, args.d_model, trainable=False, weights=[pos_encoding])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VATIJiKwFvoY"},"source":["### Position encoding lookup"]},{"cell_type":"code","metadata":{"id":"aw1eqqMzF8mB"},"source":["inputs = train_enc_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wclw8-yiF3Gr"},"source":["# make position (0...n_seq)\n","position = tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True)\n","position"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAKiUQYuGECw"},"source":["# embedding lookup\n","embed = embedding(position)\n","embed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jpGewMmEINq"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"IfIQaEslDFPP"},"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Positional Embedding Class\n","    \"\"\"\n","    def __init__(self, args, name='position_embedding'):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","        \n","        pos_encoding = PositionalEmbedding.get_sinusoid_encoding(args.n_seq, args.d_model)\n","        self.embedding = tf.keras.layers.Embedding(args.n_seq, args.d_model, trainable=False, weights=[pos_encoding])\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: 입력\n","        :return embed: positional embedding lookup 결과\n","        \"\"\"\n","        # make position (0...n_seq)\n","        position = tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True)\n","        position = tf.cast(position, tf.int32)\n","        # embedding lookup\n","        embed = self.embedding(position)\n","        return embed\n","\n","    @staticmethod\n","    def get_sinusoid_encoding(n_seq, d_model):\n","        \"\"\"\n","        sinusoid encoding 생성\n","        :param n_seq: sequence number\n","        :param n_seq: model hidden dimension\n","        :return: positional encoding table\n","        \"\"\"\n","        # calculate exp\n","        exs = np.array([2 * (i_ang // 2) / d_model for i_ang in range(d_model)])\n","        # calculate power\n","        angles = np.power(10000, exs)\n","        # make position\n","        pos = np.array([[i] for i in range(n_seq)])\n","        # position angle\n","        pos_encoding = pos / angles\n","        # sin even number\n","        pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n","        # print(pos_encoding)\n","        # cos odd number\n","        pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n","        # print(pos_encoding)\n","        return tf.cast(pos_encoding, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9P_eFKxH26ei"},"source":["# position encoding 확인\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(6, 8)\n","pos_encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDMcqbPp3ipc"},"source":["# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, args.d_model))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uuUYZeeDWjc"},"source":["# PositionalEmbedding 클래스 시험\n","pos_embedding = PositionalEmbedding(args)\n","dec_pos = pos_embedding(train_enc_inputs)\n","dec_pos.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzu-44yIEfEO"},"source":["# 512x512 position encoding table 생성\n","pos_encoding = PositionalEmbedding.get_sinusoid_encoding(512, 512)\n","# display\n","plt.pcolormesh(pos_encoding, cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgmPfzaBIo_V"},"source":["# Transformer"]},{"cell_type":"code","metadata":{"id":"cjgxweKGGeNZ"},"source":["embedding = SharedEmbedding(args)\n","position = PositionalEmbedding(args)\n","        \n","encoder_layers = [EncoderLayer(args, name=f'encoder_layer_{i}') for i in range(args.n_layer)]\n","decoder_layers = [DecoderLayer(args, name=f'decoder_layer_{i}') for i in range(args.n_layer)]\n","\n","dropout = tf.keras.layers.Dropout(args.dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qs12GpNoGjlF"},"source":["enc_tokens = train_enc_inputs\n","dec_tokens = train_dec_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJIv9PL4HXcO"},"source":["# encoder self attention mask\n","enc_self_mask = get_pad_mask(enc_tokens, args.i_pad)\n","enc_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfJnOL35HakM"},"source":["# decoder self attention mask\n","dec_self_mask = get_causal_mask(dec_tokens, args.i_pad)\n","dec_self_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dv4vBDpzHUp1"},"source":["# encoder and decoder attention mask\n","enc_dec_mask = get_pad_mask(enc_tokens, args.i_pad)\n","enc_dec_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JE59up7OMlv9"},"source":["# enc_tokens embedding lookup\n","enc_hidden = embedding(enc_tokens) + position(enc_tokens)\n","enc_hidden = dropout(enc_hidden)\n","enc_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsYeYgRcMtWw"},"source":["# call encoder layers\n","for encoder_layer in encoder_layers:\n","    enc_hidden = encoder_layer((enc_hidden, enc_self_mask))\n","enc_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oR9yhpfGMyIi"},"source":["# dec_tokens embedding lookup\n","dec_hidden = embedding(dec_tokens) + position(dec_tokens)\n","dec_hidden = dropout(dec_hidden)\n","dec_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqFEMfwUM6mp"},"source":["# call decoder layers\n","for decoder_layer in decoder_layers:\n","    dec_hidden = decoder_layer((dec_hidden, enc_hidden, dec_self_mask, enc_dec_mask))\n","dec_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nIMkU3ALM-Yz"},"source":["# call weight shared embedding (model=linear)\n","logits = embedding(dec_hidden, mode='linear')\n","logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtnO5WUuGWEr"},"source":["### 실습\n","- 아래 Class를 완성하세요."]},{"cell_type":"code","metadata":{"id":"NjUj8dKbLBPV"},"source":["class Transformer(tf.keras.Model):\n","    \"\"\"\n","    Transformer Class\n","    \"\"\"\n","    def __init__(self, args, name='transformer'):\n","        \"\"\"\n","        생성자\n","        :param args: Args 객체\n","        :param name: layer name\n","        \"\"\"\n","        super().__init__(name=name)\n","\n","        self.i_pad = args.i_pad\n","        self.embedding = SharedEmbedding(args)\n","        self.position = PositionalEmbedding(args)\n","        \n","        self.encoder_layers = [EncoderLayer(args, name=f'encoder_layer_{i}') for i in range(args.n_layer)]\n","        self.decoder_layers = [DecoderLayer(args, name=f'decoder_layer_{i}') for i in range(args.n_layer)]\n","\n","        self.dropout = tf.keras.layers.Dropout(args.dropout)\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        layer 실행\n","        :param inputs: enc_tokens, dec_tokens tuple\n","        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n","        \"\"\"\n","        enc_tokens, dec_tokens = inputs\n","        # encoder self attention mask\n","        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n","        # decoder self attention mask\n","        dec_self_mask = get_causal_mask(dec_tokens, self.i_pad)\n","        # encoder and decoder attention mask\n","        enc_dec_mask = get_pad_mask(enc_tokens, self.i_pad)\n","\n","        # enc_tokens, dec_tokens embedding lookup\n","        enc_hidden = self.embedding(enc_tokens) + self.position(enc_tokens)\n","        enc_hidden = self.dropout(enc_hidden)\n","\n","        # call encoder layers\n","        for encoder_layer in self.encoder_layers:\n","            enc_hidden = encoder_layer((enc_hidden, enc_self_mask))\n","        \n","        # dec_tokens embedding lookup\n","        dec_hidden = self.embedding(dec_tokens) + self.position(dec_tokens)\n","        dec_hidden = self.dropout(dec_hidden)\n","\n","        # call decoder layers\n","        for decoder_layer in self.decoder_layers:\n","            dec_hidden = decoder_layer((dec_hidden, enc_hidden, dec_self_mask, enc_dec_mask))\n","\n","        # call weight shared embedding (model=linear)\n","        logits = self.embedding(dec_hidden, mode='linear')\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbcZGCC2L-MB"},"source":["# Transformer 기능 확인. 최종 결과가 (bs, n_seq(dec), n_vocab)\n","transformer = Transformer(args)\n","logits = transformer((train_enc_inputs, train_dec_inputs))\n","logits.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FaJEgxT2nRjm"},"source":["# 실습\n","- 지금까지 작성한 Transformer의 구성요소를 정리해서 아래에 Transformer 모델을 완성하세요."]}]}